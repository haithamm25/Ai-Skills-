{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":658267,"sourceType":"datasetVersion","datasetId":277323},{"sourceId":13948619,"sourceType":"datasetVersion","datasetId":8890503}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"libraries","metadata":{}},{"cell_type":"code","source":"from re import sub\nfrom skimage.io import  imread, imshow\nfrom skimage.transform import  resize, rescale\nfrom skimage.color import rgb2gray\nimport numpy as np\nfrom os import listdir, path\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os,os.path\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications import MobileNetV2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:15:16.872800Z","iopub.execute_input":"2025-12-01T17:15:16.873139Z","iopub.status.idle":"2025-12-01T17:15:16.882111Z","shell.execute_reply.started":"2025-12-01T17:15:16.873111Z","shell.execute_reply":"2025-12-01T17:15:16.880723Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"preprocessing","metadata":{}},{"cell_type":"code","source":"SEED = 42\nIMG_SIZE = (224, 224)  # ResNet50 input size\nBATCH_SIZE = 32\nEPOCHS = 15\nLEARNING_RATE = 0.001\n\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\ndata='/kaggle/input/plantvillage-dataset/color'\n\ndef create_dataframe(data_path):\n    filepaths = []\n    labels = []\n    folds = os.listdir(data_path)\n\n    for fold in folds:\n        f_path = os.path.join(data_path, fold)\n        imgs = os.listdir(f_path)\n\n        for img in imgs:\n            img_path = os.path.join(f_path, img)\n\n            filepaths.append(img_path)\n            labels.append(fold)\n\n    fseries = pd.Series(filepaths, name='Filepaths')\n    lseries = pd.Series(labels, name='Labels')\n\n    return pd.concat([fseries, lseries], axis=1)                                         \ndf = create_dataframe(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:01:47.774535Z","iopub.execute_input":"2025-12-01T17:01:47.775551Z","iopub.status.idle":"2025-12-01T17:01:47.930907Z","shell.execute_reply.started":"2025-12-01T17:01:47.775518Z","shell.execute_reply":"2025-12-01T17:01:47.929951Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"train_df, dummy_df = train_test_split(df,  train_size= 0.8, shuffle= True, random_state= 42)\nvalid_df, test_df = train_test_split(dummy_df,  train_size= 0.5, shuffle= True, random_state= 42)\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=25,\n    width_shift_range=0.25,\n    height_shift_range=0.25,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nvalid_datagen = ImageDataGenerator(rescale=1./255) \ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepaths',\n    y_col='Labels',\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical',\n    shuffle=True,\n    seed=42\n)\nvalid_generator = valid_datagen.flow_from_dataframe(\n    dataframe=valid_df,\n    x_col='Filepaths',\n    y_col='Labels',\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical',\n    shuffle=False\n)\ntest_generator = test_datagen.flow_from_dataframe(\n    dataframe=test_df,\n    x_col='Filepaths',\n    y_col='Labels',\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical',\n    shuffle=False\n)\nlabel_encoder = LabelEncoder()\ndf['label_encoded'] = label_encoder.fit_transform(df['Labels'])\n\nnum_classes = len(label_encoder.classes_)\nprint(f\"\\nNumber of classes: {num_classes}\")\nprint(\"\\nChecking class distribution...\")\n\nclass_counts = df['Labels'].value_counts()\nprint(f\"Classes with < 3 samples: {sum(class_counts < 3)}\")\n\nvalid_classes = class_counts[class_counts >= 3].index\ndf_filtered = df[df['Labels'].isin(valid_classes)].copy()\n\nprint(f\"Original dataset: {len(df)} images, {len(df['Labels'].unique())} classes\")\nprint(f\"Filtered dataset: {len(df_filtered)} images, {len(df_filtered['Labels'].unique())} classes\")\n\nlabel_encoder = LabelEncoder()\ndf_filtered['label_encoded'] = label_encoder.fit_transform(df_filtered['Labels'])\nnum_classes = len(label_encoder.classes_)   # <-- FIXED\n\nX = df_filtered['Filepaths'].values\ny = df_filtered['label_encoded'].values\n\n\nX_train_val, X_test, y_train_val, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=SEED, stratify=y\n)\n\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train_val, y_train_val, test_size=0.2, random_state=SEED, stratify=y_train_val\n)\n\nNUM_CLASSES = len(train_generator.class_indices)\nprint(\"classes:\", train_generator.class_indices)\n\nprint(f\"\\nDataset split:\")\nprint(f\"Train: {len(X_train)} images\")\nprint(f\"Validation: {len(X_val)} images\")\nprint(f\"Test: {len(X_test)} images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:27:43.359867Z","iopub.execute_input":"2025-12-01T17:27:43.360849Z","iopub.status.idle":"2025-12-01T17:27:43.382140Z","shell.execute_reply.started":"2025-12-01T17:27:43.360817Z","shell.execute_reply":"2025-12-01T17:27:43.380748Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2436306528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# train generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m train_generator = train_datagen.flow_from_directory(\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'DATA_DIR' is not defined"],"ename":"NameError","evalue":"name 'DATA_DIR' is not defined","output_type":"error"}],"execution_count":34},{"cell_type":"code","source":"# تأكد إن عندك tensorflow 2.x\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers, models, optimizers, callbacks\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:07:59.165422Z","iopub.execute_input":"2025-12-01T17:07:59.165804Z","iopub.status.idle":"2025-12-01T17:07:59.171963Z","shell.execute_reply.started":"2025-12-01T17:07:59.165781Z","shell.execute_reply":"2025-12-01T17:07:59.170859Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"base_model = MobileNetV2(\n    input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n    include_top=False,\n    weights='/kaggle/input/weights/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5'\n)\nbase_model.trainable = False  # freeze initially\n\n# بناء الرأس (head)\ninputs = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\nx = base_model(inputs, training=False)  # important: training=False عند استخدام layers مثل BatchNorm\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dropout(0.3)(x)\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.3)(x)\noutputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs, outputs)\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:20:12.942231Z","iopub.execute_input":"2025-12-01T17:20:12.942562Z","iopub.status.idle":"2025-12-01T17:20:14.462793Z","shell.execute_reply.started":"2025-12-01T17:20:12.942539Z","shell.execute_reply":"2025-12-01T17:20:14.461255Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_6 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m327,936\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m)             │         \u001b[38;5;34m9,766\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">327,936</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,766</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,595,686\u001b[0m (9.90 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,595,686</span> (9.90 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m337,702\u001b[0m (1.29 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">337,702</span> (1.29 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n</pre>\n"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"initial_lr = 1e-3\nmodel.compile(\n    optimizer=optimizers.Adam(learning_rate=initial_lr),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nckpt_path = \"mobilenetv2_best.h5\"\ncb_list = [\n    callbacks.ModelCheckpoint(ckpt_path, monitor='val_accuracy', save_best_only=True, verbose=1),\n    callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1),\n    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:21:51.007448Z","iopub.execute_input":"2025-12-01T17:21:51.008395Z","iopub.status.idle":"2025-12-01T17:21:51.020881Z","shell.execute_reply.started":"2025-12-01T17:21:51.008357Z","shell.execute_reply":"2025-12-01T17:21:51.019772Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"EPOCHS_HEAD = 8  # عدد epochs مبدئي للـ head\nhistory_head = model.fit(\n    train_generator,\n    validation_data=valid_generator,\n    epochs=EPOCHS_HEAD,\n    callbacks=cb_list\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:29:22.861198Z","iopub.execute_input":"2025-12-01T17:29:22.861550Z","iopub.status.idle":"2025-12-01T20:43:01.192096Z","shell.execute_reply.started":"2025-12-01T17:29:22.861526Z","shell.execute_reply":"2025-12-01T20:43:01.191078Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/8\n\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6512 - loss: 1.2633\nEpoch 1: val_accuracy improved from -inf to 0.90479, saving model to mobilenetv2_best.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1572s\u001b[0m 1s/step - accuracy: 0.6513 - loss: 1.2630 - val_accuracy: 0.9048 - val_loss: 0.3148 - learning_rate: 0.0010\nEpoch 2/8\n\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 970ms/step - accuracy: 0.8336 - loss: 0.5194\nEpoch 2: val_accuracy improved from 0.90479 to 0.91731, saving model to mobilenetv2_best.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1439s\u001b[0m 1s/step - accuracy: 0.8336 - loss: 0.5194 - val_accuracy: 0.9173 - val_loss: 0.2605 - learning_rate: 0.0010\nEpoch 3/8\n\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961ms/step - accuracy: 0.8521 - loss: 0.4565\nEpoch 3: val_accuracy improved from 0.91731 to 0.92026, saving model to mobilenetv2_best.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1426s\u001b[0m 1s/step - accuracy: 0.8521 - loss: 0.4565 - val_accuracy: 0.9203 - val_loss: 0.2399 - learning_rate: 0.0010\nEpoch 4/8\n\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932ms/step - accuracy: 0.8576 - loss: 0.4295\nEpoch 4: val_accuracy improved from 0.92026 to 0.92431, saving model to mobilenetv2_best.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1388s\u001b[0m 1s/step - accuracy: 0.8576 - loss: 0.4295 - val_accuracy: 0.9243 - val_loss: 0.2185 - learning_rate: 0.0010\nEpoch 5/8\n\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971ms/step - accuracy: 0.8633 - loss: 0.4224\nEpoch 5: val_accuracy improved from 0.92431 to 0.93241, saving model to mobilenetv2_best.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1461s\u001b[0m 1s/step - accuracy: 0.8633 - loss: 0.4224 - val_accuracy: 0.9324 - val_loss: 0.2065 - learning_rate: 0.0010\nEpoch 6/8\n\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972ms/step - accuracy: 0.8695 - loss: 0.4025\nEpoch 6: val_accuracy did not improve from 0.93241\n\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1439s\u001b[0m 1s/step - accuracy: 0.8695 - loss: 0.4025 - val_accuracy: 0.9184 - val_loss: 0.2455 - learning_rate: 0.0010\nEpoch 7/8\n\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 976ms/step - accuracy: 0.8705 - loss: 0.4023\nEpoch 7: val_accuracy did not improve from 0.93241\n\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1444s\u001b[0m 1s/step - accuracy: 0.8705 - loss: 0.4023 - val_accuracy: 0.9258 - val_loss: 0.2144 - learning_rate: 0.0010\nEpoch 8/8\n\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 978ms/step - accuracy: 0.8721 - loss: 0.3963\nEpoch 8: val_accuracy did not improve from 0.93241\n\nEpoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1447s\u001b[0m 1s/step - accuracy: 0.8721 - loss: 0.3963 - val_accuracy: 0.9177 - val_loss: 0.2358 - learning_rate: 0.0010\nRestoring model weights from the end of the best epoch: 5.\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"model.save(\"mobilenetv2.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T20:52:06.305434Z","iopub.execute_input":"2025-12-01T20:52:06.305915Z","iopub.status.idle":"2025-12-01T20:52:06.848049Z","shell.execute_reply.started":"2025-12-01T20:52:06.305889Z","shell.execute_reply":"2025-12-01T20:52:06.846844Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}