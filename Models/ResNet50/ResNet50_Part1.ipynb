{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd5c0b1",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e5f28ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchinfo import summary\n",
    "import cv2\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from PIL import Image\n",
    "import warnings\n",
    "from matplotlib.patches import Rectangle\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b95a71",
   "metadata": {},
   "source": [
    "Data splitting (Train / Validation / Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbbdebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# NOTE: The data path has been set to your specified Windows location.\n",
    "DATA_DIR = r\"C:\\Users\\mirab\\OneDrive\\Documents\\junior\\basic of AI programming\\archive.zip\\plantvillage dataset\\color\" \n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 20 # Start with a small number, increase for full training\n",
    "SPLIT_RATIOS = {'train': 0.7, 'val': 0.2, 'test': 0.1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912ace36",
   "metadata": {},
   "source": [
    "Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb3b97f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classes found: 38\n",
      "Class names: ['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Blueberry___healthy', 'Cherry_(including_sour)___Powdery_mildew', 'Cherry_(including_sour)___healthy', 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot', 'Corn_(maize)___Common_rust_', 'Corn_(maize)___Northern_Leaf_Blight', 'Corn_(maize)___healthy', 'Grape___Black_rot', 'Grape___Esca_(Black_Measles)', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Grape___healthy', 'Orange___Haunglongbing_(Citrus_greening)', 'Peach___Bacterial_spot', 'Peach___healthy', 'Pepper,_bell___Bacterial_spot', 'Pepper,_bell___healthy', 'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Raspberry___healthy', 'Soybean___healthy', 'Squash___Powdery_mildew', 'Strawberry___Leaf_scorch', 'Strawberry___healthy', 'Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy']\n",
      "Total images in dataset: 54305\n",
      "\n",
      "--- Dataset Split Summary ---\n",
      "Train set size: 38013 (70.0%)\n",
      "Validation set size: 10861 (20.0%)\n",
      "Test set size: 5431 (10.0%)\n"
     ]
    }
   ],
   "source": [
    "# Define data transforms with required augmentations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.RandomHorizontalFlip(), # Orientation Augmentation\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # Color Augmentation\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Load the full dataset\n",
    "try:\n",
    "    full_dataset = datasets.ImageFolder(DATA_DIR)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset from {DATA_DIR}. Please check the path and file structure.\")\n",
    "    print(f\"Error details: {e}\")\n",
    "    # Exit or use dummy data if loading fails\n",
    "    raise\n",
    "\n",
    "class_names = full_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"Total classes found: {num_classes}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "print(f\"Total images in dataset: {len(full_dataset)}\")\n",
    "\n",
    "# Get indices for the full dataset\n",
    "all_indices = list(range(len(full_dataset)))\n",
    "\n",
    "# Calculate split sizes\n",
    "train_size = int(SPLIT_RATIOS['train'] * len(full_dataset))\n",
    "val_size = int(SPLIT_RATIOS['val'] * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size # Remaining for test\n",
    "\n",
    "# 1. Split Train/Temp (70% / 30%)\n",
    "train_indices, temp_indices = train_test_split(\n",
    "    all_indices, \n",
    "    train_size=train_size, \n",
    "    test_size=val_size + test_size, \n",
    "    stratify=full_dataset.targets, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Split Val/Test from the remaining (20% / 10%)\n",
    "val_ratio = val_size / (val_size + test_size)\n",
    "\n",
    "val_indices, test_indices = train_test_split(\n",
    "    temp_indices, \n",
    "    train_size=val_ratio, \n",
    "    stratify=[full_dataset.targets[i] for i in temp_indices], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create Subsets with appropriate transforms\n",
    "train_dataset = Subset(datasets.ImageFolder(DATA_DIR, transform=data_transforms['train']), train_indices)\n",
    "val_dataset = Subset(datasets.ImageFolder(DATA_DIR, transform=data_transforms['val']), val_indices)\n",
    "test_dataset = Subset(datasets.ImageFolder(DATA_DIR, transform=data_transforms['test']), test_indices)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0) # num_workers=0 for Windows compatibility\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"\\n--- Dataset Split Summary ---\")\n",
    "print(f\"Train set size: {len(train_dataset)} ({SPLIT_RATIOS['train']*100}%)\")\n",
    "print(f\"Validation set size: {len(val_dataset)} ({SPLIT_RATIOS['val']*100}%)\")\n",
    "print(f\"Test set size: {len(test_dataset)} ({SPLIT_RATIOS['test']*100}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32d7f09",
   "metadata": {},
   "source": [
    "Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1cffe2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model setup complete. Ready for training.\n"
     ]
    }
   ],
   "source": [
    "def get_resnet50_model(num_classes):\n",
    "    # Load pre-trained ResNet50\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "    # Freeze all the parameters in the network\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Replace the final fully connected layer with a new one for our number of classes\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    \n",
    "    # Unfreeze the final layer and the last few convolutional blocks for fine-tuning\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    # Unfreeze the parameters of the last convolutional block (layer4)\n",
    "    for param in model.layer4.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    return model\n",
    "\n",
    "model = get_resnet50_model(num_classes).to(DEVICE)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Use a lower learning rate for fine-tuning\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE) \n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) # Decay LR every 5 epochs\n",
    "\n",
    "print(\"Model setup complete. Ready for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dacf1a",
   "metadata": {},
   "source": [
    "Accuracy Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb2357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    epochs = range(1, len(history['train_acc']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(epochs, history['train_acc'])\n",
    "    plt.plot(epochs, history['val_acc'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfb213b",
   "metadata": {},
   "source": [
    "Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f52d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_history(history):\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(epochs, history['train_loss'])\n",
    "    plt.plot(epochs, history['val_loss'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1ac53",
   "metadata": {},
   "source": [
    "Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c175d9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\miniconda3\\envs\\ai4beg\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\pc\\miniconda3\\envs\\ai4beg\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "bn1 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "relu ReLU(inplace=True)\n",
      "maxpool MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "layer1 Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "layer2 Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (3): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "layer3 Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (3): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (4): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (5): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "layer4 Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "avgpool AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "fc Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "backbone = models.resnet50(pretrained=True)\n",
    "for name, layer in backbone.named_children():\n",
    "    print(name, layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4beg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
